{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering player lookup table. This may take a moment.\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Errors encountered:\n",
      "(746190, 'Error getting home runs')\n",
      "(746190, 'Error getting away runs')\n",
      "\n",
      "=================================================\n",
      "Gamelogs generated. Now collecting odds data.\n",
      "=================================================\n",
      "Updated gamelogs/game_747163.csv with over/under runline.\n",
      "1\n",
      "Updated gamelogs/game_745381.csv with over/under runline.\n",
      "2\n",
      "Updated gamelogs/game_746919.csv with over/under runline.\n",
      "3\n",
      "Updated gamelogs/game_747000.csv with over/under runline.\n",
      "4\n",
      "Updated gamelogs/game_746598.csv with over/under runline.\n",
      "5\n",
      "Updated gamelogs/game_746436.csv with over/under runline.\n",
      "6\n",
      "Updated gamelogs/game_746677.csv with over/under runline.\n",
      "7\n",
      "Updated gamelogs/game_745544.csv with over/under runline.\n",
      "8\n",
      "Updated gamelogs/game_745053.csv with over/under runline.\n",
      "9\n",
      "Updated gamelogs/game_745790.csv with over/under runline.\n",
      "10\n",
      "Updated gamelogs/game_746759.csv with over/under runline.\n",
      "11\n",
      "Updated gamelogs/game_745951.csv with over/under runline.\n",
      "12\n",
      "Updated gamelogs/game_746188.csv with over/under runline.\n",
      "13\n",
      "Updated gamelogs/game_745301.csv with over/under runline.\n",
      "14\n",
      "Updated gamelogs/game_745783.csv with over/under runline.\n",
      "15\n",
      "Updated gamelogs/game_746437.csv with over/under runline.\n",
      "16\n",
      "Updated gamelogs/game_745950.csv with over/under runline.\n",
      "17\n",
      "Updated gamelogs/game_745298.csv with over/under runline.\n",
      "18\n",
      "Updated gamelogs/game_747002.csv with over/under runline.\n",
      "19\n",
      "Updated gamelogs/game_745545.csv with over/under runline.\n",
      "20\n",
      "Updated gamelogs/game_744980.csv with over/under runline.\n",
      "21\n",
      "Updated gamelogs/game_746843.csv with over/under runline.\n",
      "22\n",
      "Updated gamelogs/game_746674.csv with over/under runline.\n",
      "23\n",
      "Updated gamelogs/game_745462.csv with over/under runline.\n",
      "24\n",
      "Updated gamelogs/game_745542.csv with over/under runline.\n",
      "25\n",
      "Updated gamelogs/game_746438.csv with over/under runline.\n",
      "26\n",
      "Updated gamelogs/game_745054.csv with over/under runline.\n",
      "27\n",
      "Updated gamelogs/game_747001.csv with over/under runline.\n",
      "28\n",
      "Updated gamelogs/game_745784.csv with over/under runline.\n",
      "29\n",
      "Updated gamelogs/game_744974.csv with over/under runline.\n",
      "30\n",
      "Updated gamelogs/game_746357.csv with over/under runline.\n",
      "31\n",
      "Updated gamelogs/game_745949.csv with over/under runline.\n",
      "32\n",
      "Updated gamelogs/game_745144.csv with over/under runline.\n",
      "33\n",
      "Updated gamelogs/game_746518.csv with over/under runline.\n",
      "34\n",
      "Updated gamelogs/game_746189.csv with over/under runline.\n",
      "35\n",
      "Updated gamelogs/game_745463.csv with over/under runline.\n",
      "36\n",
      "Updated gamelogs/game_746434.csv with over/under runline.\n",
      "37\n",
      "Updated gamelogs/game_746841.csv with over/under runline.\n",
      "38\n",
      "Updated gamelogs/game_745049.csv with over/under runline.\n",
      "39\n",
      "Updated gamelogs/game_745785.csv with over/under runline.\n",
      "40\n",
      "Updated gamelogs/game_745537.csv with over/under runline.\n",
      "41\n",
      "Updated gamelogs/game_746675.csv with over/under runline.\n",
      "42\n",
      "Updated gamelogs/game_746997.csv with over/under runline.\n",
      "43\n",
      "Updated gamelogs/game_744977.csv with over/under runline.\n",
      "44\n",
      "Updated gamelogs/game_745626.csv with over/under runline.\n",
      "45\n",
      "Updated gamelogs/game_746355.csv with over/under runline.\n",
      "46\n",
      "Updated gamelogs/game_745140.csv with over/under runline.\n",
      "47\n",
      "Updated gamelogs/game_745952.csv with over/under runline.\n",
      "48\n",
      "Updated gamelogs/game_746515.csv with over/under runline.\n",
      "49\n",
      "Updated gamelogs/game_746190.csv with over/under runline.\n",
      "50\n",
      "\n",
      "Games with duplicate dates:\n",
      "\n",
      "=================================================\n",
      "Odds data added. Now adding custom stats.\n",
      "=================================================\n",
      "Processed and saved game stats for game 744820 to gamelogs/gamestats_744820.csv\n",
      "Processed and saved game stats for game 746269 to gamelogs/gamestats_746269.csv\n",
      "Processed and saved game stats for game 745873 to gamelogs/gamestats_745873.csv\n",
      "Processed and saved game stats for game 745955 to gamelogs/gamestats_745955.csv\n",
      "Processed and saved game stats for game 745056 to gamelogs/gamestats_745056.csv\n",
      "Processed and saved game stats for game 746761 to gamelogs/gamestats_746761.csv\n",
      "Stats file for nan not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 747164 to gamelogs/gamestats_747164.csv\n",
      "Processed and saved game stats for game 746520 to gamelogs/gamestats_746520.csv\n",
      "Processed and saved game stats for game 746112 to gamelogs/gamestats_746112.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745219 to gamelogs/gamestats_745219.csv\n",
      "Processed and saved game stats for game 744816 to gamelogs/gamestats_744816.csv\n",
      "Processed and saved game stats for game 746923 to gamelogs/gamestats_746923.csv\n",
      "Processed and saved game stats for game 745711 to gamelogs/gamestats_745711.csv\n",
      "Processed and saved game stats for game 744895 to gamelogs/gamestats_744895.csv\n",
      "Processed and saved game stats for game 745057 to gamelogs/gamestats_745057.csv\n",
      "Processed and saved game stats for game 746025 to gamelogs/gamestats_746025.csv\n",
      "Processed and saved game stats for game 745872 to gamelogs/gamestats_745872.csv\n",
      "Processed and saved game stats for game 745956 to gamelogs/gamestats_745956.csv\n",
      "Processed and saved game stats for game 746517 to gamelogs/gamestats_746517.csv\n",
      "Processed and saved game stats for game 745303 to gamelogs/gamestats_745303.csv\n",
      "Processed and saved game stats for game 746114 to gamelogs/gamestats_746114.csv\n",
      "Stats file for nan not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 747166 to gamelogs/gamestats_747166.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745216 to gamelogs/gamestats_745216.csv\n",
      "Processed and saved game stats for game 746603 to gamelogs/gamestats_746603.csv\n",
      "Processed and saved game stats for game 746682 to gamelogs/gamestats_746682.csv\n",
      "Processed and saved game stats for game 745058 to gamelogs/gamestats_745058.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746921 to gamelogs/gamestats_746921.csv\n",
      "Processed and saved game stats for game 745874 to gamelogs/gamestats_745874.csv\n",
      "Processed and saved game stats for game 746762 to gamelogs/gamestats_746762.csv\n",
      "Processed and saved game stats for game 745953 to gamelogs/gamestats_745953.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746192 to gamelogs/gamestats_746192.csv\n",
      "Processed and saved game stats for game 747165 to gamelogs/gamestats_747165.csv\n",
      "Processed and saved game stats for game 745383 to gamelogs/gamestats_745383.csv\n",
      "Processed and saved game stats for game 745300 to gamelogs/gamestats_745300.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 747004 to gamelogs/gamestats_747004.csv\n",
      "Processed and saved game stats for game 746601 to gamelogs/gamestats_746601.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746435 to gamelogs/gamestats_746435.csv\n",
      "Processed and saved game stats for game 746673 to gamelogs/gamestats_746673.csv\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 745548 to gamelogs/gamestats_745548.csv\n",
      "Processed and saved game stats for game 745055 to gamelogs/gamestats_745055.csv\n",
      "Processed and saved game stats for game 745788 to gamelogs/gamestats_745788.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746924 to gamelogs/gamestats_746924.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745867 to gamelogs/gamestats_745867.csv\n",
      "Processed and saved game stats for game 746757 to gamelogs/gamestats_746757.csv\n",
      "Processed and saved game stats for game 745954 to gamelogs/gamestats_745954.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746187 to gamelogs/gamestats_746187.csv\n",
      "Processed and saved game stats for game 747160 to gamelogs/gamestats_747160.csv\n",
      "Processed and saved game stats for game 745382 to gamelogs/gamestats_745382.csv\n",
      "Processed and saved game stats for game 745302 to gamelogs/gamestats_745302.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745869 to gamelogs/gamestats_745869.csv\n",
      "Processed and saved game stats for game 747163 to gamelogs/gamestats_747163.csv\n",
      "Processed and saved game stats for game 745381 to gamelogs/gamestats_745381.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746919 to gamelogs/gamestats_746919.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 747000 to gamelogs/gamestats_747000.csv\n",
      "Processed and saved game stats for game 746598 to gamelogs/gamestats_746598.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746436 to gamelogs/gamestats_746436.csv\n",
      "Processed and saved game stats for game 746677 to gamelogs/gamestats_746677.csv\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 745544 to gamelogs/gamestats_745544.csv\n",
      "Processed and saved game stats for game 745053 to gamelogs/gamestats_745053.csv\n",
      "Processed and saved game stats for game 745790 to gamelogs/gamestats_745790.csv\n",
      "Processed and saved game stats for game 746759 to gamelogs/gamestats_746759.csv\n",
      "Processed and saved game stats for game 745951 to gamelogs/gamestats_745951.csv\n",
      "Processed and saved game stats for game 746188 to gamelogs/gamestats_746188.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 745301 to gamelogs/gamestats_745301.csv\n",
      "Processed and saved game stats for game 745783 to gamelogs/gamestats_745783.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 746437 to gamelogs/gamestats_746437.csv\n",
      "Processed and saved game stats for game 745950 to gamelogs/gamestats_745950.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 745298 to gamelogs/gamestats_745298.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 747002 to gamelogs/gamestats_747002.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 745545 to gamelogs/gamestats_745545.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 744980 to gamelogs/gamestats_744980.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746843 to gamelogs/gamestats_746843.csv\n",
      "Processed and saved game stats for game 746674 to gamelogs/gamestats_746674.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745462 to gamelogs/gamestats_745462.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 745542 to gamelogs/gamestats_745542.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746438 to gamelogs/gamestats_746438.csv\n",
      "Processed and saved game stats for game 745054 to gamelogs/gamestats_745054.csv\n",
      "Processed and saved game stats for game 747001 to gamelogs/gamestats_747001.csv\n",
      "Processed and saved game stats for game 745784 to gamelogs/gamestats_745784.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 744974 to gamelogs/gamestats_744974.csv\n",
      "Processed and saved game stats for game 746357 to gamelogs/gamestats_746357.csv\n",
      "Processed and saved game stats for game 745949 to gamelogs/gamestats_745949.csv\n",
      "Processed and saved game stats for game 745144 to gamelogs/gamestats_745144.csv\n",
      "Processed and saved game stats for game 746518 to gamelogs/gamestats_746518.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746189 to gamelogs/gamestats_746189.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745463 to gamelogs/gamestats_745463.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746434 to gamelogs/gamestats_746434.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746841 to gamelogs/gamestats_746841.csv\n",
      "Processed and saved game stats for game 745049 to gamelogs/gamestats_745049.csv\n",
      "Processed and saved game stats for game 745785 to gamelogs/gamestats_745785.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Stats file for nan not found (pitching).\n",
      "Processed and saved game stats for game 745537 to gamelogs/gamestats_745537.csv\n",
      "Processed and saved game stats for game 746675 to gamelogs/gamestats_746675.csv\n",
      "Processed and saved game stats for game 746997 to gamelogs/gamestats_746997.csv\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 744977 to gamelogs/gamestats_744977.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Stats file for unknown not found (pitching).\n",
      "Processed and saved game stats for game 745626 to gamelogs/gamestats_745626.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746355 to gamelogs/gamestats_746355.csv\n",
      "Processed and saved game stats for game 745140 to gamelogs/gamestats_745140.csv\n",
      "Processed and saved game stats for game 745952 to gamelogs/gamestats_745952.csv\n",
      "Stats file for unknown not found (batting).\n",
      "Processed and saved game stats for game 746515 to gamelogs/gamestats_746515.csv\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Stats file for nan not found (batting).\n",
      "Processed and saved game stats for game 746190 to gamelogs/gamestats_746190.csv\n",
      "\n",
      "=================================================\n",
      "Custom stats added. Now generating dataset.\n",
      "=================================================\n",
      "First pass: columns collected\n",
      "Second pass: data added.\n",
      "Master dataset saved to model/unsorted_currentdata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
      "C:\\Users\\kesse\\AppData\\Local\\Temp\\ipykernel_25344\\3683890415.py:680: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved successfully.\n",
      "Updated dataset saved successfully.\n",
      "\n",
      "=================================================\n",
      "SUCCESS - THIS FILE IS COMPLETE\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pybaseball import playerid_reverse_lookup\n",
    "\n",
    "# Load game_pks.csv to get team names and IDs\n",
    "game_pks_df = pd.read_csv('game_pks.csv')\n",
    "\n",
    "# Mapping of team 3-digit IDs to oddshark 5-digit IDs \n",
    "team_to_oddshark_id = {\n",
    "    120: 27017, 146: 27022, 139: 27003, 144: 27009, 140: 27002, 117: 27023,\n",
    "    135: 26996, 143: 26995, 110: 27008, 136: 27011, 121: 27014, 109: 27007,\n",
    "    108: 26998, 133: 27016, 141: 27010, 114: 27014, 138: 27019, 142: 27005,\n",
    "    116: 26999, 147: 27001, 137: 26997, 118: 27006, 145: 27018, 115: 27004,\n",
    "    111: 27021, 119: 27015, 112: 27020, 158: 27012, 113: 27000, 134: 27013\n",
    "}\n",
    "\n",
    "errors = []\n",
    "\n",
    "def get_game_data(gamepk):\n",
    "    url = f\"https://statsapi.mlb.com/api/v1.1/game/{gamepk}/feed/live\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_bbref_id(mlbam_id):\n",
    "    try:\n",
    "        lookup_df = playerid_reverse_lookup([mlbam_id], key_type='mlbam')\n",
    "        bbref_id = lookup_df.loc[lookup_df['key_mlbam'] == mlbam_id, 'key_bbref'].values[0]\n",
    "        return bbref_id\n",
    "    except IndexError:\n",
    "        return 'unknown'\n",
    "\n",
    "def extract_starting_lineup(game_data, team_side):\n",
    "    lineup = {}\n",
    "    team = game_data['liveData']['boxscore']['teams'][team_side]['players']\n",
    "    \n",
    "    for player_id, player_info in team.items():\n",
    "        try:\n",
    "            if 'battingOrder' in player_info and int(player_info['battingOrder']) % 100 == 0:\n",
    "                order = int(player_info['battingOrder']) // 100\n",
    "                mlbam_id = player_info['person']['id']\n",
    "                bbref_id = get_bbref_id(mlbam_id)\n",
    "                lineup[order] = {\n",
    "                    'name': player_info['person']['fullName'],\n",
    "                    'mlbam_id': mlbam_id,\n",
    "                    'bbref_id': bbref_id\n",
    "                }\n",
    "        except Exception as e:\n",
    "            errors.append((gamepk, f\"Error processing player {player_id} in {team_side} lineup: {str(e)}\"))\n",
    "    \n",
    "    # Ensure lineup is filled and sorted by batting order\n",
    "    return [lineup.get(i, {'name': '', 'mlbam_id': '', 'bbref_id': ''}) for i in range(1, 10)]\n",
    "\n",
    "def get_pitchers(game_data, team_side):\n",
    "    team = game_data['liveData']['boxscore']['teams'][team_side]\n",
    "    pitchers = []\n",
    "    for idx, pitcher_id in enumerate(team['pitchers']):\n",
    "        try:\n",
    "            pitcher = team['players'][f'ID{pitcher_id}']\n",
    "            mlbam_id = pitcher['person']['id']\n",
    "            bbref_id = get_bbref_id(mlbam_id)\n",
    "            pitchers.append({\n",
    "                'name': pitcher['person']['fullName'],\n",
    "                'mlbam_id': mlbam_id,\n",
    "                'bbref_id': bbref_id,\n",
    "                'order': idx + 1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            errors.append((gamepk, f\"Error processing pitcher {pitcher_id} in {team_side} team: {str(e)}\"))\n",
    "    return pitchers\n",
    "\n",
    "def get_bullpen(game_data, team_side):\n",
    "    team = game_data['liveData']['boxscore']['teams'][team_side]\n",
    "    bullpen = []\n",
    "    if team['pitchers']:  # Game already played\n",
    "        for pitcher_id in team['bullpen'] + team['pitchers'][1:]:\n",
    "            try:\n",
    "                pitcher = team['players'][f'ID{pitcher_id}']\n",
    "                mlbam_id = pitcher['person']['id']\n",
    "                bbref_id = get_bbref_id(mlbam_id)\n",
    "                bullpen.append({\n",
    "                    'name': pitcher['person']['fullName'],\n",
    "                    'mlbam_id': mlbam_id,\n",
    "                    'bbref_id': bbref_id\n",
    "                })\n",
    "            except Exception as e:\n",
    "                errors.append((gamepk, f\"Error processing bullpen pitcher {pitcher_id} in {team_side} team: {str(e)}\"))\n",
    "    else:  # Game not yet played\n",
    "        for pitcher_id in team['bullpen']:\n",
    "            try:\n",
    "                pitcher = team['players'][f'ID{pitcher_id}']\n",
    "                mlbam_id = pitcher['person']['id']\n",
    "                bbref_id = get_bbref_id(mlbam_id)\n",
    "                bullpen.append({\n",
    "                    'name': pitcher['person']['fullName'],\n",
    "                    'mlbam_id': mlbam_id,\n",
    "                    'bbref_id': bbref_id\n",
    "                })\n",
    "            except Exception as e:\n",
    "                errors.append((gamepk, f\"Error processing bullpen pitcher {pitcher_id} in {team_side} team: {str(e)}\"))\n",
    "    return bullpen\n",
    "\n",
    "def create_game_dataframe(gamepk):\n",
    "    game_data = get_game_data(gamepk)\n",
    "    \n",
    "    try:\n",
    "        home_lineup = extract_starting_lineup(game_data, 'home')\n",
    "        away_lineup = extract_starting_lineup(game_data, 'away')\n",
    "        home_bullpen = get_bullpen(game_data, 'home')\n",
    "        away_bullpen = get_bullpen(game_data, 'away')\n",
    "    except Exception as e:\n",
    "        errors.append((gamepk, f\"Error extracting lineups: {str(e)}\"))\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        home_pitchers = get_pitchers(game_data, 'home')\n",
    "        away_pitchers = get_pitchers(game_data, 'away')\n",
    "    except Exception as e:\n",
    "        errors.append((gamepk, f\"Error extracting pitchers: {str(e)}\"))\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get additional game information\n",
    "    game_info = game_pks_df[game_pks_df['game_id'] == gamepk].iloc[0]\n",
    "    \n",
    "    try:\n",
    "        game_date = game_info['game_date']\n",
    "    except KeyError:\n",
    "        game_date = 'unknown'\n",
    "        errors.append((gamepk, \"Error getting game date\"))\n",
    "    \n",
    "    try:\n",
    "        runs_home = game_data['liveData']['linescore']['teams']['home']['runs']\n",
    "    except KeyError:\n",
    "        runs_home = 0\n",
    "        errors.append((gamepk, \"Error getting home runs\"))\n",
    "    \n",
    "    try:\n",
    "        runs_away = game_data['liveData']['linescore']['teams']['away']['runs']\n",
    "    except KeyError:\n",
    "        runs_away = 0\n",
    "        errors.append((gamepk, \"Error getting away runs\"))\n",
    "    \n",
    "    runs_total = runs_home + runs_away\n",
    "    \n",
    "    # Get team information from game_pks.csv\n",
    "    home_id = game_info['home_id']\n",
    "    away_id = game_info['away_id']\n",
    "    home_name = game_info['home_name']\n",
    "    away_name = game_info['away_name']\n",
    "    \n",
    "    # Check for invalid team IDs\n",
    "    if home_id not in team_to_oddshark_id or away_id not in team_to_oddshark_id:\n",
    "        print(f\"Invalid team IDs - {home_name} {home_id} vs {away_name} {away_id}\")\n",
    "    \n",
    "    home_oddshark_id = team_to_oddshark_id.get(home_id, 'unknown')\n",
    "    away_oddshark_id = team_to_oddshark_id.get(away_id, 'unknown')\n",
    "    \n",
    "    game_record = {\n",
    "        'game_id': gamepk, 'game_date': game_date, 'runs_home': runs_home, 'runs_away': runs_away, 'runs_total': runs_total,\n",
    "        'home_id': home_id, 'home_name': home_name, 'away_id': away_id, 'away_name': away_name,\n",
    "        'home_oddshark_id': home_oddshark_id, 'away_oddshark_id': away_oddshark_id\n",
    "    }\n",
    "\n",
    "    for i, player in enumerate(away_lineup, start=1):\n",
    "        game_record[f'Away_Batter{i}_Name'] = player['name']\n",
    "        game_record[f'Away_Batter{i}_ID'] = player['mlbam_id']\n",
    "        game_record[f'Away_Batter{i}_bbrefID'] = player['bbref_id']\n",
    "        \n",
    "    for i, player in enumerate(home_lineup, start=1):\n",
    "        game_record[f'Home_Batter{i}_Name'] = player['name']\n",
    "        game_record[f'Home_Batter{i}_ID'] = player['mlbam_id']\n",
    "        game_record[f'Home_Batter{i}_bbrefID'] = player['bbref_id']\n",
    "    \n",
    "    # Add starting pitchers\n",
    "    if home_pitchers:\n",
    "        game_record['Home_SP_Name'] = home_pitchers[0]['name']\n",
    "        game_record['Home_SP_ID'] = home_pitchers[0]['mlbam_id']\n",
    "        game_record['Home_SP_bbrefID'] = home_pitchers[0]['bbref_id']\n",
    "    \n",
    "    if away_pitchers:\n",
    "        game_record['Away_SP_Name'] = away_pitchers[0]['name']\n",
    "        game_record['Away_SP_ID'] = away_pitchers[0]['mlbam_id']\n",
    "        game_record['Away_SP_bbrefID'] = away_pitchers[0]['bbref_id']\n",
    "    \n",
    "    # Add bullpen pitchers\n",
    "    for i, pitcher in enumerate(home_bullpen, start=1):\n",
    "        game_record[f'Home_bullpen_{i}_Name'] = pitcher['name']\n",
    "        game_record[f'Home_bullpen_{i}_ID'] = pitcher['mlbam_id']\n",
    "        game_record[f'Home_bullpen_{i}_bbrefID'] = pitcher['bbref_id']\n",
    "    \n",
    "    for i, pitcher in enumerate(away_bullpen, start=1):\n",
    "        game_record[f'Away_bullpen_{i}_Name'] = pitcher['name']\n",
    "        game_record[f'Away_bullpen_{i}_ID'] = pitcher['mlbam_id']\n",
    "        game_record[f'Away_bullpen_{i}_bbrefID'] = pitcher['bbref_id']\n",
    "\n",
    "\n",
    "    # Add remaining pitchers\n",
    "    for i, pitcher in enumerate(home_pitchers[1:], start=2):\n",
    "        game_record[f'Home_P_{i}_Name'] = pitcher['name']\n",
    "        game_record[f'Home_P_{i}_ID'] = pitcher['mlbam_id']\n",
    "        game_record[f'Home_P_{i}_bbrefID'] = pitcher['bbref_id']\n",
    "    \n",
    "    for i, pitcher in enumerate(away_pitchers[1:], start=2):\n",
    "        game_record[f'Away_P_{i}_Name'] = pitcher['name']\n",
    "        game_record[f'Away_P_{i}_ID'] = pitcher['mlbam_id']\n",
    "        game_record[f'Away_P_{i}_bbrefID'] = pitcher['bbref_id']\n",
    "    \n",
    "    return pd.DataFrame([game_record])\n",
    "\n",
    "def save_game_to_csv(gamepk):\n",
    "    game_df = create_game_dataframe(gamepk)\n",
    "    if not game_df.empty:\n",
    "        file_name = f'gamelogs/game_{gamepk}.csv'\n",
    "        game_df.to_csv(file_name, index=False)\n",
    "        #print(f\"Data exported to {file_name}\")\n",
    "    else:\n",
    "        print(f\"No data exported for game {gamepk}\")\n",
    "\n",
    "all_games = pd.read_csv('game_pks.csv').game_id\n",
    "\n",
    "\n",
    "gamepks = all_games.tail(100) ################################################### CHANGE THIS #################################################################\n",
    "count = 0\n",
    "for gamepk in gamepks:\n",
    "    save_game_to_csv(gamepk)\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "\n",
    "# Print collected errors\n",
    "if errors:\n",
    "    print(\"Errors encountered:\")\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"No errors encountered.\")\n",
    "\n",
    "print('\\n=================================================\\nGamelogs generated. Now collecting odds data.\\n=================================================')\n",
    "\n",
    "# ====================================================== ODDS DATA ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_over_under_runline(oddshark_id, game_date):\n",
    "    year = game_date.year\n",
    "    url = f\"https://www.oddsshark.com/stats/gamelog/baseball/mlb/{oddshark_id}?season={year}\"\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        df = tables[0]\n",
    "    except Exception as e:\n",
    "        print(f\"BAD - error for team {oddshark_id} on date {game_date}: {e}\")\n",
    "        return 'unknown', None, None, None\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"BAD - No data in table for team {oddshark_id} on date {game_date}\")\n",
    "        return 'unknown', None, None, None\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%b %d, %Y')\n",
    "    matching_rows = df[df['Date'] == game_date]\n",
    "\n",
    "    if len(matching_rows) > 1:\n",
    "        print(f\"DOUBLEHEADER on {game_date}\")\n",
    "        return '', oddshark_id, year, game_date\n",
    "    \n",
    "    if matching_rows.empty:\n",
    "        print(f\"BAD - No matching date found for team {oddshark_id} on date {game_date}\")\n",
    "        return 'unknown', None, None, None\n",
    "    \n",
    "    over_under = matching_rows.iloc[0]['Total']\n",
    "    return over_under, None, None, None\n",
    "\n",
    "def update_gamelogs_with_over_under(game_pks_file, gamelogs_folder):\n",
    "    game_pks_df = pd.read_csv(game_pks_file)\n",
    "    game_pks = game_pks_df['game_id'].tail(50)  # Set the number of recent games to do\n",
    "    duplicates = []\n",
    "    \n",
    "    count = 0\n",
    "    for game_id in game_pks:\n",
    "        try:\n",
    "            gamelog_file = f'{gamelogs_folder}/game_{game_id}.csv'\n",
    "            gamelog_df = pd.read_csv(gamelog_file)\n",
    "            \n",
    "            home_oddshark_id = gamelog_df.loc[0, 'home_oddshark_id']\n",
    "            game_date_str = gamelog_df.loc[0, 'game_date']\n",
    "            game_date = datetime.strptime(game_date_str, '%Y-%m-%d')\n",
    "            \n",
    "            over_under_runline, duplicate_id, duplicate_year, duplicate_date = fetch_over_under_runline(home_oddshark_id, game_date)\n",
    "            \n",
    "            if duplicate_id:\n",
    "                duplicates.append((duplicate_id, duplicate_year, duplicate_date))\n",
    "                \n",
    "            gamelog_df['over_under_runline'] = over_under_runline\n",
    "             \n",
    "            gamelog_df.to_csv(gamelog_file, index=False)\n",
    "            print(f\"Updated {gamelog_file} with over/under runline.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating gamelog for game_id {game_id}: {e}\")\n",
    "\n",
    "        count+=1\n",
    "        if count % 100:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"\\nGames with duplicate dates:\")\n",
    "    for dup in duplicates:\n",
    "        print(f\"Team Oddshark ID: {dup[0]}, Year: {dup[1]}, Date: {dup[2].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Update the paths as necessary\n",
    "game_pks_file = 'game_pks.csv'\n",
    "gamelogs_folder = 'gamelogs'\n",
    "\n",
    "# Run the update function\n",
    "update_gamelogs_with_over_under(game_pks_file, gamelogs_folder)\n",
    "\n",
    "print('\\n=================================================\\nOdds data added. Now adding custom stats.\\n=================================================')\n",
    "\n",
    "# ============================================ CUSTOM STATS =======================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_player_stats(bbref_id, player_type, game_id):\n",
    "    \"\"\"\n",
    "    Get the player's stats for the specific game_id. If not available, return the most recent stats.\n",
    "    \"\"\"\n",
    "    stats_dir = 'batters' if player_type == 'batting' else 'pitchers'\n",
    "    stats_file = os.path.join(stats_dir, f'{bbref_id}_stats_{player_type}.csv')\n",
    "    \n",
    "    if not os.path.exists(stats_file):\n",
    "        print(f\"Stats file for {bbref_id} not found ({player_type}).\")\n",
    "        return None\n",
    "    \n",
    "    stats_df = pd.read_csv(stats_file)\n",
    "    game_stats = stats_df[stats_df['game_id'] == game_id]\n",
    "    \n",
    "    if not game_stats.empty:\n",
    "        return game_stats.iloc[0]\n",
    "    else:\n",
    "        return stats_df.iloc[-1]\n",
    "\n",
    "def process_game(game_id):\n",
    "    # Read the gamelog file\n",
    "    game_file = f'gamelogs/game_{game_id}.csv'\n",
    "    if not os.path.exists(game_file):\n",
    "        print(f\"Gamelog file for game {game_id} not found.\")\n",
    "        return\n",
    "    \n",
    "    game_df = pd.read_csv(game_file)\n",
    "    game_data = game_df.iloc[0].to_dict()\n",
    "    \n",
    "    # Define relevant columns for batters and pitchers\n",
    "    batter_columns = ['AVG_20', 'OBP_20', 'SLG_20', 'OPS_20', 'SB_20', 'CS_20', 'XB_20', 'TB_20', 'SO_20',\n",
    "                      'AVG_10', 'OBP_10', 'SLG_10', 'OPS_10', 'SB_10', 'CS_10', 'XB_10', 'TB_10', 'SO_10',\n",
    "                      'AVG_5', 'OBP_5', 'SLG_5', 'OPS_5', 'SB_5', 'CS_5', 'XB_5', 'TB_5', 'SO_5',\n",
    "                      'AVG_3', 'OBP_3', 'SLG_3', 'OPS_3', 'SB_3', 'CS_3', 'XB_3', 'TB_3', 'SO_3']\n",
    "    pitcher_columns = ['IP_real_20', 'ERA', 'H_20', 'BF_20', 'HR_20', 'R_20', 'ER_20', 'BB_20', 'SO_20', 'XB_against_20',\n",
    "                       'TB_against_20', 'ERA_20', 'WHIP_20', 'IP_real_10', 'H_10', 'BF_10', 'HR_10', 'R_10', 'ER_10', 'BB_10', 'SO_10', 'XB_against_10',\n",
    "                       'TB_against_10', 'ERA_10', 'WHIP_10', 'IP_real_5', 'H_5', 'BF_5', 'HR_5', 'R_5', 'ER_5', 'BB_5',\n",
    "                       'SO_5', 'XB_against_5', 'TB_against_5', 'ERA_5', 'WHIP_5', 'IP_real_3', 'H_3', 'BF_3', 'HR_3', 'R_3', 'ER_3', 'BB_3',\n",
    "                       'SO_3', 'XB_against_3', 'TB_against_3', 'ERA_3', 'WHIP_3']\n",
    "    \n",
    "    # Fetch stats for each batter\n",
    "    for i in range(1, 10):\n",
    "        for team in ['Away', 'Home']:\n",
    "            bbref_id = game_data.get(f'{team}_Batter{i}_bbrefID')\n",
    "            if bbref_id:\n",
    "                stats = get_player_stats(bbref_id, 'batting', game_id)\n",
    "                if stats is not None:\n",
    "                    for col in batter_columns:\n",
    "                        game_data[f'{team}_Batter{i}_{col}'] = stats.get(col, '')\n",
    "            else:\n",
    "                print(f'missing bbrefID for game {game_id}')\n",
    "\n",
    "    # Fetch stats for each pitcher\n",
    "    for team in ['Away', 'Home']:\n",
    "        for i in range(1, 11):\n",
    "            role = 'SP' if i == 1 else f'P_{i}'\n",
    "            bbref_id = game_data.get(f'{team}_{role}_bbrefID')\n",
    "            if bbref_id:\n",
    "                stats = get_player_stats(bbref_id, 'pitching', game_id)\n",
    "                if stats is not None:\n",
    "                    for col in pitcher_columns:\n",
    "                        game_data[f'{team}_{role}_{col}'] = stats.get(col, '')\n",
    "\n",
    "    # Fetch stats for each bullpen pitcher\n",
    "    for team in ['Away', 'Home']:\n",
    "        for i in range(1, 15):  # Adjust the range according to your maximum expected number of bullpen pitchers\n",
    "            role = f'bullpen_{i}'\n",
    "            bbref_id = game_data.get(f'{team}_{role}_bbrefID')\n",
    "            if bbref_id:\n",
    "                stats = get_player_stats(bbref_id, 'pitching', game_id)\n",
    "                if stats is not None:\n",
    "                    for col in pitcher_columns:\n",
    "                        game_data[f'{team}_{role}_{col}'] = stats.get(col, '')\n",
    "    \n",
    "    # Create a DataFrame from the updated game data\n",
    "    updated_game_df = pd.DataFrame([game_data])\n",
    "    \n",
    "    # Save the updated game data to a new CSV file\n",
    "    output_file = f'gamelogs/gamestats_{game_id}.csv'\n",
    "    updated_game_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed and saved game stats for game {game_id} to {output_file}\")\n",
    "\n",
    "def process_recent_games(num_recent_games):\n",
    "    game_pks_file = 'game_pks.csv'\n",
    "    if not os.path.exists(game_pks_file):\n",
    "        print(f\"{game_pks_file} not found.\")\n",
    "        return\n",
    "\n",
    "    game_pks_df = pd.read_csv(game_pks_file)\n",
    "    recent_game_pks = game_pks_df.tail(num_recent_games)['game_id'].tolist()\n",
    "    \n",
    "    for game_id in recent_game_pks:\n",
    "        process_game(game_id)\n",
    "\n",
    "# Input the number of most recent games to process\n",
    "num_recent_games = 100  ######################################################### CHANGE THIS ##########################################################\n",
    "process_recent_games(num_recent_games)\n",
    "\n",
    "print('\\n=================================================\\nCustom stats added. Now generating dataset.\\n=================================================')\n",
    "\n",
    "# ================================================= GENERATING DATASET ===========================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "game_pks_path = 'game_pks.csv'\n",
    "gamelogs_dir = 'gamelogs/'\n",
    "output_path = 'model/unsorted_currentdata.csv'\n",
    "\n",
    "# Read the game_pks.csv file\n",
    "game_pks_df = pd.read_csv(game_pks_path).tail(100)\n",
    "game_pks_list = game_pks_df['game_id'].tolist()\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Initialize a set to store all columns\n",
    "all_columns = set()\n",
    "\n",
    "# First pass: Collect all unique columns\n",
    "for game_pk in game_pks_list:\n",
    "    file_path = os.path.join(gamelogs_dir, f'gamestats_{game_pk}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_columns.update(df.columns)\n",
    "    else:\n",
    "        print(f\"BAD - File {file_path} not found.\")\n",
    "\n",
    "print(\"First pass: columns collected\")\n",
    "\n",
    "# Second pass: Read files and align columns\n",
    "for game_pk in game_pks_list:\n",
    "    file_path = os.path.join(gamelogs_dir, f'gamestats_{game_pk}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Add missing columns with default value of NaN (handled by reindex)\n",
    "        df = df.reindex(columns=all_columns)\n",
    "        dataframes.append(df)\n",
    "\n",
    "print(\"Second pass: data added.\")\n",
    "\n",
    "# Concatenate all the DataFrames\n",
    "master_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the master DataFrame to a CSV file\n",
    "master_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Master dataset saved to {output_path}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('model/unsorted_currentdata.csv', low_memory=False)\n",
    "\n",
    "pitcher_columns = [\n",
    "    'IP_real_20', 'ERA', 'H_20', 'BF_20', 'HR_20', 'R_20', 'ER_20', 'BB_20', 'SO_20', 'XB_against_20',\n",
    "    'TB_against_20', 'ERA_20', 'WHIP_20', 'IP_real_10', 'H_10', 'BF_10', 'HR_10', 'R_10', 'ER_10', 'BB_10', 'SO_10', 'XB_against_10',\n",
    "    'TB_against_10', 'ERA_10', 'WHIP_10', 'IP_real_5', 'H_5', 'BF_5', 'HR_5', 'R_5', 'ER_5', 'BB_5',\n",
    "    'SO_5', 'XB_against_5', 'TB_against_5', 'ERA_5', 'WHIP_5', 'IP_real_3', 'H_3', 'BF_3', 'HR_3', 'R_3', 'ER_3', 'BB_3',\n",
    "    'SO_3', 'XB_against_3', 'TB_against_3', 'ERA_3', 'WHIP_3'\n",
    "]\n",
    "    \n",
    "\n",
    "# Add 'over_under_runline' column right after 'runs_total'\n",
    "if 'over_under_runline' in df.columns:\n",
    "    columns = df.columns.tolist()\n",
    "    runline_index = columns.index('over_under_runline')\n",
    "    columns.insert(columns.index('runs_total') + 1, columns.pop(runline_index))\n",
    "    df = df[columns]\n",
    "else:\n",
    "    print(\"Warning: 'over_under_runline' column not found.\")\n",
    "\n",
    "# Define a function to sort columns by player order and then alphabetically\n",
    "def sort_columns(df):\n",
    "    # List to store sorted column names\n",
    "    sorted_columns = []\n",
    "    \n",
    "    # Ensure the specified order of the first few general columns\n",
    "    first_columns = ['gamepk','game_id', 'game_date', 'home_name', 'away_name', 'runs_home', 'runs_away', 'runs_total', 'over_under_runline']\n",
    "    for col in first_columns:\n",
    "        if col in df.columns:\n",
    "            sorted_columns.append(col)\n",
    "    \n",
    "    # Lists to categorize columns\n",
    "    away_batter_columns = [[] for _ in range(9)]\n",
    "    home_batter_columns = [[] for _ in range(9)]\n",
    "    away_pitcher_columns = [[] for _ in range(9)]\n",
    "    home_pitcher_columns = [[] for _ in range(9)]\n",
    "    away_bullpen_columns = [[] for _ in range(15)]\n",
    "    home_bullpen_columns = [[] for _ in range(15)]\n",
    "    \n",
    "    # Helper function to ensure the list is long enough\n",
    "    def ensure_length(lst, index):\n",
    "        while len(lst) <= index:\n",
    "            lst.append([])\n",
    "    \n",
    "    # Helper function to sort player-specific columns\n",
    "    def sort_player_columns(columns):\n",
    "        player_columns = []\n",
    "        other_columns = []\n",
    "        for col in columns:\n",
    "            if any(key in col for key in ['Name', 'ID', 'bbrefID']):\n",
    "                player_columns.append(col)\n",
    "            else:\n",
    "                other_columns.append(col)\n",
    "        return sorted(player_columns) + sorted(other_columns)\n",
    "    \n",
    "    # Categorize columns\n",
    "    for col in df.columns:\n",
    "        if col.startswith('Away_Batter'):\n",
    "            try:\n",
    "                num = int(col.split('_')[1][6]) - 1\n",
    "                ensure_length(away_batter_columns, num)\n",
    "                away_batter_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        elif col.startswith('Home_Batter'):\n",
    "            try:\n",
    "                num = int(col.split('_')[1][6]) - 1\n",
    "                ensure_length(home_batter_columns, num)\n",
    "                home_batter_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        elif col.startswith('Away_P_') or col.startswith('Away_SP'):\n",
    "            try:\n",
    "                if 'SP' in col:\n",
    "                    num = 0\n",
    "                else:\n",
    "                    num = int(col.split('_')[2])\n",
    "                ensure_length(away_pitcher_columns, num)\n",
    "                away_pitcher_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        elif col.startswith('Home_P_') or col.startswith('Home_SP'):\n",
    "            try:\n",
    "                if 'SP' in col:\n",
    "                    num = 0\n",
    "                else:\n",
    "                    num = int(col.split('_')[2])\n",
    "                ensure_length(home_pitcher_columns, num)\n",
    "                home_pitcher_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        elif col.startswith('Away_bullpen'):\n",
    "            try:\n",
    "                num = int(col.split('_')[2]) - 1\n",
    "                ensure_length(away_bullpen_columns, num)\n",
    "                away_bullpen_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        elif col.startswith('Home_bullpen'):\n",
    "            try:\n",
    "                num = int(col.split('_')[2]) - 1\n",
    "                ensure_length(home_bullpen_columns, num)\n",
    "                home_bullpen_columns[num].append(col)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "    # Sort each category\n",
    "    for batter_columns in away_batter_columns:\n",
    "        sorted_columns.extend(sort_player_columns(batter_columns))\n",
    "    for batter_columns in home_batter_columns:\n",
    "        sorted_columns.extend(sort_player_columns(batter_columns))\n",
    "    for pitcher_columns in away_pitcher_columns:\n",
    "        sorted_columns.extend(sort_player_columns(pitcher_columns))\n",
    "    for pitcher_columns in home_pitcher_columns:\n",
    "        sorted_columns.extend(sort_player_columns(pitcher_columns))\n",
    "    for bullpen_columns in away_bullpen_columns:\n",
    "        sorted_columns.extend(sort_player_columns(bullpen_columns))\n",
    "    for bullpen_columns in home_bullpen_columns:\n",
    "        sorted_columns.extend(sort_player_columns(bullpen_columns))\n",
    "    \n",
    "    return df[sorted_columns]\n",
    "\n",
    "# Sort columns in the dataset\n",
    "sorted_df = sort_columns(df)\n",
    "\n",
    "# Save the sorted dataset\n",
    "sorted_df.to_csv('model/currentdata.csv', index=False)\n",
    "\n",
    "# Load the sorted dataset\n",
    "sorted_df = pd.read_csv('model/currentdata.csv', low_memory=False)\n",
    "\n",
    "# Function to determine if a column should be removed\n",
    "def should_remove_column(col):\n",
    "    if col.startswith('Home_bullpen_') or col.startswith('Away_bullpen_'):\n",
    "        try:\n",
    "            # Extract the number from the column name and check if it is 16 or higher\n",
    "            number = int(col.split('_')[2])\n",
    "            return number >= 16\n",
    "        except ValueError:\n",
    "            # If the suffix is not numeric, do not remove\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# Remove columns that start with 'Home_bullpen_' or 'Away_bullpen_' and have a number 16 or higher\n",
    "columns_to_remove = [col for col in sorted_df.columns if should_remove_column(col)]\n",
    "sorted_df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "\n",
    "# Remove rows where runline is 'unknown'\n",
    "filtered_df = sorted_df[sorted_df['over_under_runline'] != 'unknown']\n",
    "\n",
    "# Function to filter games based on date range\n",
    "def filter_games_by_date(df):\n",
    "    # Convert game_date to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "    \n",
    "    # Filter out games before April 5 and after October 5\n",
    "    filtered_df = df[(df['game_date'].dt.month >= 4) & (df['game_date'].dt.month <= 10) &\n",
    "                     ((df['game_date'].dt.month != 4) | (df['game_date'].dt.day >= 5)) &\n",
    "                     ((df['game_date'].dt.month != 10) | (df['game_date'].dt.day <= 5))]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Filter the games by date\n",
    "filtered_df = filter_games_by_date(filtered_df)\n",
    "\n",
    "# Convert over_under_runline to numeric\n",
    "filtered_df['over_under_runline'] = pd.to_numeric(filtered_df['over_under_runline'])\n",
    "\n",
    "# Create binary target variable\n",
    "filtered_df['over_under_target'] = (filtered_df['runs_total'] >= filtered_df['over_under_runline']).astype(int)\n",
    "\n",
    "# Rearrange columns to move 'over_under_target' to the right of 'over_under_runline'\n",
    "columns = list(filtered_df.columns)\n",
    "over_under_runline_index = columns.index('over_under_runline')\n",
    "\n",
    "# Insert 'over_under_target' right after 'over_under_runline'\n",
    "columns.insert(over_under_runline_index + 1, columns.pop(columns.index('over_under_target')))\n",
    "filtered_df = filtered_df[columns]\n",
    "\n",
    "\n",
    "# Save the filtered dataset with the target variable\n",
    "filtered_df.to_csv('model/currentdata.csv', index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'model/currentdata.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List of pitcher stats columns to be averaged\n",
    "pitcher_columns = [\n",
    "    'IP_real_20', 'ERA', 'H_20', 'BF_20', 'HR_20', 'R_20', 'ER_20', 'BB_20', 'SO_20', 'XB_against_20',\n",
    "    'TB_against_20', 'ERA_20', 'WHIP_20', 'IP_real_10', 'H_10', 'BF_10', 'HR_10', 'R_10', 'ER_10', 'BB_10', 'SO_10', 'XB_against_10',\n",
    "    'TB_against_10', 'ERA_10', 'WHIP_10', 'IP_real_5', 'H_5', 'BF_5', 'HR_5', 'R_5', 'ER_5', 'BB_5',\n",
    "    'SO_5', 'XB_against_5', 'TB_against_5', 'ERA_5', 'WHIP_5', 'IP_real_3', 'H_3', 'BF_3', 'HR_3', 'R_3', 'ER_3', 'BB_3',\n",
    "    'SO_3', 'XB_against_3', 'TB_against_3', 'ERA_3', 'WHIP_3'\n",
    "]\n",
    "\n",
    "# Function to calculate average stats for bullpen pitchers\n",
    "def calculate_bullpen_averages(team_prefix):\n",
    "    for stat in pitcher_columns:\n",
    "        stat_columns = [f\"{team_prefix}_bullpen_{i}_{stat}\" for i in range(1, 13)]\n",
    "        # Convert columns to numeric, coercing errors to NaNs\n",
    "        df[stat_columns] = df[stat_columns].apply(pd.to_numeric, errors='coerce')\n",
    "        # Replace infinite values with NaNs\n",
    "        df[stat_columns] = df[stat_columns].replace([np.inf, -np.inf], np.nan)\n",
    "        df[f\"{team_prefix}_bullpen_avg_{stat}\"] = df[stat_columns].mean(axis=1)\n",
    "\n",
    "# Calculate averages for home and away teams\n",
    "calculate_bullpen_averages('Home')\n",
    "calculate_bullpen_averages('Away')\n",
    "\n",
    "# Save the updated dataframe back to the same file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Updated dataset saved successfully.\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'model/currentdata.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List of pitcher stats columns to be removed\n",
    "pitcher_columns = [\n",
    "    'IP_real_20', 'ERA', 'H_20', 'BF_20', 'HR_20', 'R_20', 'ER_20', 'BB_20', 'SO_20', 'XB_against_20',\n",
    "    'TB_against_20', 'ERA_20', 'WHIP_20', 'IP_real_10', 'H_10', 'BF_10', 'HR_10', 'R_10', 'ER_10', 'BB_10', 'SO_10', 'XB_against_10',\n",
    "    'TB_against_10', 'ERA_10', 'WHIP_10', 'IP_real_5', 'H_5', 'BF_5', 'HR_5', 'R_5', 'ER_5', 'BB_5',\n",
    "    'SO_5', 'XB_against_5', 'TB_against_5', 'ERA_5', 'WHIP_5', 'IP_real_3', 'H_3', 'BF_3', 'HR_3', 'R_3', 'ER_3', 'BB_3',\n",
    "    'SO_3', 'XB_against_3', 'TB_against_3', 'ERA_3', 'WHIP_3'\n",
    "]\n",
    "\n",
    "# Function to remove individual bullpen columns\n",
    "def remove_bullpen_columns(team_prefix):\n",
    "    for stat in pitcher_columns:\n",
    "        stat_columns = [f\"{team_prefix}_bullpen_{i}_{stat}\" for i in range(1, 13)]\n",
    "        df.drop(columns=stat_columns, inplace=True)\n",
    "\n",
    "# Remove bullpen columns for home and away teams\n",
    "remove_bullpen_columns('Home')\n",
    "remove_bullpen_columns('Away')\n",
    "\n",
    "# Save the updated dataframe back to the same file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Updated dataset saved successfully.\")\n",
    "\n",
    "print('\\n=================================================\\nSUCCESS - THIS FILE IS COMPLETE\\n=================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
